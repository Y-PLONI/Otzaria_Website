import mongoose from 'mongoose';
import fs from 'fs';
import readline from 'readline';
import path from 'path';
import dotenv from 'dotenv';
import slugify from 'slugify';
import { fileURLToPath } from 'url';

// 1. ×˜×¢×™× ×ª ××©×ª× ×™ ×¡×‘×™×‘×”
dotenv.config({ path: '.env.local' });
dotenv.config({ path: '.env' });

const FILES_JSON_PATH = 'files.json';
const MESSAGES_JSON_PATH = 'messages.json';
const BACKUPS_JSON_PATH = 'backups.json';

// --- ×¡×›××•×ª (×”×¢×ª×§ ××“×•×™×§ ××”×¤×¨×•×™×§×˜ ×”×—×“×©) ---
const UserSchema = new mongoose.Schema({
    name: { type: String, required: true, unique: true },
    email: { type: String, required: true, unique: true },
    password: { type: String, required: true },
    role: { type: String, enum: ['user', 'admin'], default: 'user' },
    points: { type: Number, default: 0 },
}, { timestamps: true });

const BookSchema = new mongoose.Schema({
    name: { type: String, required: true, unique: true },
    slug: { type: String, index: true },
    totalPages: { type: Number, default: 0 },
    completedPages: { type: Number, default: 0 },
    category: { type: String, default: '×›×œ×œ×™' },
    folderPath: { type: String },
}, { timestamps: true });

const PageSchema = new mongoose.Schema({
    book: { type: mongoose.Schema.Types.ObjectId, ref: 'Book', required: true },
    pageNumber: { type: Number, required: true },
    content: { type: String, default: '' },
    status: { type: String, enum: ['available', 'in-progress', 'completed'], default: 'available' },
    claimedBy: { type: mongoose.Schema.Types.ObjectId, ref: 'User' },
    claimedAt: { type: Date },
    completedAt: { type: Date },
    imagePath: { type: String, required: true },
}, { timestamps: true });

const UploadSchema = new mongoose.Schema({
    uploader: { type: mongoose.Schema.Types.ObjectId, ref: 'User', required: true },
    bookName: { type: String, required: true },
    originalFileName: { type: String },
    content: { type: String }, // <--- ×–×” ×”×©×“×” ×”×§×¨×™×˜×™ ×œ×”×•×¨×“×”!
    status: { type: String, enum: ['pending', 'approved', 'rejected'], default: 'pending' },
    reviewedBy: { type: mongoose.Schema.Types.ObjectId, ref: 'User' },
}, { timestamps: true });

const MessageSchema = new mongoose.Schema({
    sender: { type: mongoose.Schema.Types.ObjectId, ref: 'User', required: true },
    recipient: { type: mongoose.Schema.Types.ObjectId, ref: 'User', default: null },
    subject: { type: String, default: '×œ×œ× × ×•×©×' },
    content: { type: String, required: true },
    isRead: { type: Boolean, default: false },
    replies: [{
      sender: { type: mongoose.Schema.Types.ObjectId, ref: 'User' },
      content: String,
      createdAt: { type: Date, default: Date.now }
    }]
}, { timestamps: true });

const User = mongoose.models.User || mongoose.model('User', UserSchema);
const Book = mongoose.models.Book || mongoose.model('Book', BookSchema);
const Page = mongoose.models.Page || mongoose.model('Page', PageSchema);
const Upload = mongoose.models.Upload || mongoose.model('Upload', UploadSchema);
const Message = mongoose.models.Message || mongoose.model('Message', MessageSchema);

// --- ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×œ× ×™×§×•×™ ×•×–×™×”×•×™ ×©××•×ª ---

// ××¤×¢× ×— ×©××•×ª ××§×•×“×“×™× (URI Component)
function decodeFileName(encodedName) {
    try {
        const uriComponent = encodedName.replace(/_/g, '%');
        return decodeURIComponent(uriComponent);
    } catch (e) { return encodedName; }
}

// ×™×•×¦×¨ "××¤×ª×— × ×§×™" ×œ×”×©×•×•××” ×‘×™×Ÿ ×©××•×ª ×§×‘×¦×™×
// ××¡×™×¨ ×¡×™×•××•×ª, ××¡×¤×¨×™× ××§×¨××™×™×, ×¨×•×•×—×™× ×•×›×•' ×›×“×™ ×œ××¦×•× ×”×ª×××”
function normalizeKey(filename) {
    if (!filename) return '';
    let name = filename;
    
    // ×× ×”×©× ××§×•×“×“ (××ª×—×™×œ ×‘-_D7...), × ×¤×¢× ×— ××•×ª×• ×§×•×“×
    if (name.startsWith('_')) {
        name = decodeFileName(name);
    }

    return name
        .replace(/\.txt$/i, '')           // ×”×¡×¨×ª ×¡×™×•××ª
        .replace(/_\d{10,}/, '')          // ×”×¡×¨×ª timestamp ××¨×•×š ×‘×¡×•×£ (×œ××©×œ _1767489134475)
        .replace(/[-_\s]+/g, ' ')         // ×”×—×œ×¤×ª ××¤×¨×™×“×™× ×‘×¨×•×•×—
        .trim()
        .toLowerCase();
}

// ×™×¦×™×¨×ª slug ×©××©××¨ ×¢×‘×¨×™×ª
function createHebrewSlug(name) {
    if (!name) return 'unknown';
    return name.trim().replace(/\s+/g, '-').replace(/[^\w\u0590-\u05FF\-]/g, '');
}

async function loadDataFromFile(filePath) {
    if (!fs.existsSync(filePath)) {
        console.warn(`âš ï¸ File not found: ${filePath}`);
        return [];
    }

    const results = [];
    const fileStream = fs.createReadStream(filePath);
    const rl = readline.createInterface({ input: fileStream, crlfDelay: Infinity });

    console.log(`ğŸ“– Streaming ${filePath}...`);

    for await (const line of rl) {
        if (!line.trim()) continue;
        try {
            const doc = JSON.parse(line);
            results.push(doc);
        } catch (err) {}
    }

    if (results.length === 0) {
        try {
            const content = fs.readFileSync(filePath, 'utf8');
            const data = JSON.parse(content);
            if (Array.isArray(data)) return data;
            return [data];
        } catch (e) {}
    }
    return results;
}

// ××¤×•×ª ×’×œ×•×‘×œ×™×•×ª
const userMap = new Map(); 
const bookMap = new Map(); 
const contentMap = new Map(); // NormalizedKey -> Content

async function restore() {
    try {
        console.log('ğŸ”Œ Connecting to MongoDB...');
        await mongoose.connect(process.env.MONGODB_URI);
        console.log('âœ… Connected.');

        // ×§×¨×™××ª ×›×œ ×”×§×‘×¦×™×
        const rawFiles = await loadDataFromFile(FILES_JSON_PATH);
        const rawBackups = await loadDataFromFile(BACKUPS_JSON_PATH);
        const rawMessages = await loadDataFromFile(MESSAGES_JSON_PATH);
        const allMetadataSources = [...rawFiles, ...rawBackups];

        // ---------------------------------------------------------
        // ×©×œ×‘ 0: ×‘× ×™×™×ª ××™× ×“×§×¡ ×ª×•×›×Ÿ ×—×›×
        // ---------------------------------------------------------
        console.log('ğŸ“ Building content index...');
        rawFiles.filter(f => f.path && f.path.startsWith('data/content/')).forEach(fileRecord => {
            if (fileRecord.data && fileRecord.data.content) {
                const rawName = path.basename(fileRecord.path); 
                const normalized = normalizeKey(rawName);
                
                // ×©××™×¨×” ×‘××¤×” ×œ×¤×™ ×”××¤×ª×— ×”×× ×•×¨××œ
                // ×× ×™×© ×›×¤×™×œ×•×™×•×ª, ×”××—×¨×•×Ÿ ×“×•×¨×¡ (×‘×“×¨×š ×›×œ×œ ×–×” ×‘×¡×“×¨)
                contentMap.set(normalized, fileRecord.data.content);
                
                // ×©××™×¨×” ×’× ×œ×¤×™ ×”×©× ×”××§×•×¨×™ ×œ×™×ª×¨ ×‘×™×˜×—×•×Ÿ
                contentMap.set(rawName, fileRecord.data.content);
            }
        });
        console.log(`âœ… Indexed ${contentMap.size} text contents.`);

        // ---------------------------------------------------------
        // ×©×œ×‘ 1: ××©×ª××©×™×
        // ---------------------------------------------------------
        const usersEntry = rawFiles.find(f => f.path === 'data/users.json');
        if (usersEntry && usersEntry.data) {
            console.log(`Processing users...`);
            for (const u of usersEntry.data) {
                const newId = new mongoose.Types.ObjectId();
                const existingUser = await User.findOne({ email: u.email });
                const finalId = existingUser ? existingUser._id : newId;
                
                userMap.set(u.id, finalId);
                const points = u.points?.$numberInt ? parseInt(u.points.$numberInt) : (u.points || 0);

                await User.updateOne(
                    { email: u.email },
                    {
                        $set: {
                            name: u.name,
                            password: u.password,
                            role: u.role,
                            points: points,
                            createdAt: u.createdAt ? new Date(u.createdAt) : new Date(),
                        },
                        $setOnInsert: { _id: finalId }
                    },
                    { upsert: true }
                );
            }
            console.log('âœ… Users imported.');
        }

        // ---------------------------------------------------------
        // ×©×œ×‘ 2: ×¡×¤×¨×™×
        // ---------------------------------------------------------
        const booksEntry = rawFiles.find(f => f.path === 'data/books.json');
        if (booksEntry && booksEntry.data) {
            console.log(`Processing books...`);
            for (const b of booksEntry.data) {
                const newId = new mongoose.Types.ObjectId();
                const slug = createHebrewSlug(b.name);
                
                const existingBook = await Book.findOne({ name: b.name });
                const finalId = existingBook ? existingBook._id : newId;

                bookMap.set(b.name, { _id: finalId, slug });
                const totalPages = b.totalPages?.$numberInt ? parseInt(b.totalPages.$numberInt) : (b.totalPages || 0);

                await Book.updateOne(
                    { name: b.name },
                    {
                        $set: {
                            slug: slug,
                            totalPages: totalPages,
                            folderPath: `/uploads/books/${slug}`,
                            createdAt: b.createdAt ? new Date(b.createdAt) : new Date()
                        },
                        $setOnInsert: { _id: finalId, completedPages: 0 }
                    },
                    { upsert: true }
                );
            }
            console.log('âœ… Books imported.');
        }

        // ---------------------------------------------------------
        // ×©×œ×‘ 3: Uploads (×§×‘×¦×™ ×˜×§×¡×˜ ×œ×”×•×¨×“×”)
        // ---------------------------------------------------------
        // ××™×ª×•×¨ ×¨×©×™××ª ×”×”×¢×œ××•×ª
        let uploadsData = [];
        rawFiles.forEach(f => {
            if (f.data && f.data.uploads && Array.isArray(f.data.uploads)) {
                uploadsData = uploadsData.concat(f.data.uploads);
            }
        });

        if (uploadsData.length > 0) {
            console.log(`ğŸ“‚ Processing ${uploadsData.length} uploads...`);
            await Upload.deleteMany({}); 

            let mappedCount = 0;
            const uploadsToInsert = uploadsData.map(u => {
                let uploaderId = null;
                if (u.uploadedById && userMap.has(u.uploadedById)) {
                    uploaderId = userMap.get(u.uploadedById);
                } else {
                    uploaderId = userMap.values().next().value;
                }

                // --- ×—×™×¤×•×© ×ª×•×›×Ÿ "×¢×§×©×Ÿ" ---
                let realContent = '';
                const keysToTry = [
                    normalizeKey(u.fileName),           // ×œ×¤×™ ×©× ×”×§×•×‘×¥ ×©× ×•×¦×¨ (×œ××©×œ ×¢× timestamp)
                    normalizeKey(u.originalFileName),   // ×œ×¤×™ ×”×©× ×”××§×•×¨×™
                    u.fileName,                         // ×œ×¤×™ ×”×©× ×”××“×•×™×§
                    u.bookName                          // ×œ×¤×™ ×©× ×”×¡×¤×¨ (×œ×¤×¢××™× ×–×” ××•×ª×• ×“×‘×¨)
                ];

                for (const key of keysToTry) {
                    if (key && contentMap.has(key)) {
                        realContent = contentMap.get(key);
                        mappedCount++;
                        break;
                    }
                }

                if (!realContent) {
                    // × ×¡×™×•×Ÿ ××—×¨×•×Ÿ: ×—×™×¤×•×© ×—×œ×§×™ ×‘××¤×”
                    // ×–×” ××™×˜×™ ×™×•×ª×¨, ××‘×œ ××¦×™×œ × ×ª×•× ×™× ×‘××§×¨×™× ×§×©×™×
                    for (const [mapKey, mapContent] of contentMap.entries()) {
                         if (mapKey.includes(normalizeKey(u.bookName)) || normalizeKey(u.fileName).includes(mapKey)) {
                             realContent = mapContent;
                             mappedCount++;
                             break;
                         }
                    }
                }

                const finalContent = realContent || "×©×’×™××”: ×”×ª×•×›×Ÿ ×œ× × ××¦× ×‘×§×•×‘×¥ ×”×’×™×‘×•×™.";

                return {
                    uploader: uploaderId,
                    bookName: u.bookName || '×¡×¤×¨ ×œ×œ× ×©×',
                    originalFileName: u.originalFileName || `upload.txt`,
                    content: finalContent, // ×›××Ÿ × ×©××¨ ×”×˜×§×¡×˜ ×©×™×•×¦×’ ×‘×”×•×¨×“×”!
                    status: u.status === 'approved' ? 'approved' : u.status === 'rejected' ? 'rejected' : 'pending',
                    reviewedBy: null,
                    createdAt: u.uploadedAt ? new Date(u.uploadedAt) : new Date(),
                    updatedAt: new Date()
                };
            });

            await Upload.insertMany(uploadsToInsert);
            console.log(`âœ… Uploads imported. Successfully matched content for ${mappedCount}/${uploadsToInsert.length} files.`);
        }

        // ---------------------------------------------------------
        // ×©×œ×‘ 4: ×“×¤×™× (Pages)
        // ---------------------------------------------------------
        console.log('ğŸ§© Processing pages...');
        const mergedPages = {};

        // ××™×¡×•×£ ××˜×-×“××˜×”
        allMetadataSources.filter(f => f.path && f.path.startsWith('data/pages/')).forEach(fileRecord => {
            const bookName = path.basename(fileRecord.path, '.json');
            if (!fileRecord.data || !Array.isArray(fileRecord.data)) return;
            if (!mergedPages[bookName]) mergedPages[bookName] = {};

            const bookInfo = bookMap.get(bookName);
            const slugForThumb = bookInfo ? bookInfo.slug : createHebrewSlug(bookName);

            fileRecord.data.forEach(p => {
                const num = p.number?.$numberInt ? parseInt(p.number.$numberInt) : p.number;
                const defaultThumb = `/uploads/books/${slugForThumb}/page.${num}.jpg`;
                
                // ×’× ×›××Ÿ × ×©×ª××© ×‘××™×¤×•×™ ×”×ª×•×›×Ÿ ×”×—×›×
                const contentKey = normalizeKey(`${bookName} page ${num}`);
                const content = contentMap.get(contentKey) || '';

                mergedPages[bookName][num] = {
                    ...mergedPages[bookName][num],
                    status: p.status,
                    claimedById: p.claimedById,
                    claimedAt: p.claimedAt,
                    completedAt: p.completedAt,
                    thumbnail: p.thumbnail || defaultThumb,
                    content: content
                };
            });
        });

        // ×™×¦×™×¨×” ×•×”×›× ×¡×”
        const pageOperations = [];
        const bookCompletedCounts = {};

        for (const [bookName, pagesObj] of Object.entries(mergedPages)) {
            const bookInfo = bookMap.get(bookName);
            if (!bookInfo) continue;
            bookCompletedCounts[bookInfo._id] = 0;

            for (const [pageNumStr, pageData] of Object.entries(pagesObj)) {
                let claimedByNewId = null;
                if (pageData.claimedById && userMap.has(pageData.claimedById)) {
                    claimedByNewId = userMap.get(pageData.claimedById);
                } else if (pageData.claimedById) {
                     claimedByNewId = userMap.values().next().value; 
                }

                if (pageData.status === 'completed') bookCompletedCounts[bookInfo._id]++;

                pageOperations.push({
                    book: bookInfo._id,
                    pageNumber: parseInt(pageNumStr),
                    content: pageData.content || '',
                    status: pageData.status || 'available',
                    claimedBy: claimedByNewId,
                    claimedAt: pageData.claimedAt ? new Date(pageData.claimedAt) : null,
                    completedAt: pageData.completedAt ? new Date(pageData.completedAt) : null,
                    imagePath: pageData.thumbnail
                });
            }
        }

        if (pageOperations.length > 0) {
            console.log(`Inserting ${pageOperations.length} pages...`);
            await Page.deleteMany({});
            const chunkSize = 500;
            for (let i = 0; i < pageOperations.length; i += chunkSize) {
                await Page.insertMany(pageOperations.slice(i, i + chunkSize));
                process.stdout.write('.');
            }
            console.log('\nâœ… Pages imported.');
        }

        for (const [bookId, count] of Object.entries(bookCompletedCounts)) {
            await Book.findByIdAndUpdate(bookId, { completedPages: count });
        }

        // ---------------------------------------------------------
        // ×©×œ×‘ 5: ×”×•×“×¢×•×ª
        // ---------------------------------------------------------
        if (rawMessages && rawMessages.length > 0) {
            console.log(`ğŸ“¨ Importing messages...`);
            const messagesToInsert = [];
            for (const msg of rawMessages) {
                const senderId = userMap.get(msg.senderId);
                const replies = (msg.replies || []).map(r => ({
                    sender: userMap.get(r.senderId),
                    content: r.message,
                    createdAt: r.createdAt ? new Date(r.createdAt) : new Date()
                })).filter(r => r.sender);

                if (senderId) {
                    messagesToInsert.push({
                        sender: senderId,
                        subject: msg.subject || '×œ×œ× × ×•×©×',
                        content: msg.message,
                        isRead: !!msg.readAt,
                        replies: replies,
                        createdAt: msg.createdAt ? new Date(msg.createdAt) : new Date()
                    });
                }
            }
            await Message.deleteMany({});
            await Message.insertMany(messagesToInsert);
            console.log('âœ… Messages imported.');
        }

        console.log('ğŸ‰ RESTORE COMPLETE!');
        process.exit(0);

    } catch (error) {
        console.error('âŒ Error during restore:', error);
        process.exit(1);
    }
}

restore();